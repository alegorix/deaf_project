# deaf_project
# ğŸ“Œ Connecter un Kinect Ã  un Raspberry Pi pour la Traduction de la Langue des Signes

## ğŸ¯ Objectif
Utiliser un Kinect avec un Raspberry Pi pour analyser une personne parlant la langue des signes et traduire les gestes en phrases en franÃ§ais.

---

## ğŸ¯ MatÃ©riel requis
- **Kinect v1 (Xbox 360) ou Kinect v2 (Xbox One)**
- **Adaptateur secteur Kinect** (obligatoire pour les modÃ¨les Xbox 360 et One)
- **Raspberry Pi 4 ou supÃ©rieur**
- **Carte microSD avec Raspberry Pi OS installÃ©**
- **CÃ¢ble USB 2.0 ou 3.0**

---

## ğŸ”Œ 1. Brancher le Kinect au Raspberry Pi
### Kinect v1 (Xbox 360)
1. Connecter le Kinect Ã  son **adaptateur secteur**.
2. Relier le cÃ¢ble USB du Kinect Ã  un **port USB du Raspberry Pi**.

### Kinect v2 (Xbox One)
1. Utiliser lâ€™**adaptateur officiel Kinect v2**.
2. Brancher lâ€™adaptateur sur une **alimentation externe**.
3. Relier le Kinect via USB au Raspberry Pi.

---

## ğŸ› ï¸ 2. Installer les pilotes OpenKinect
Les pilotes **libfreenect** permettent dâ€™utiliser le Kinect v1 sous Linux.

### Ã‰tapes dâ€™installation
1. Mettre Ã  jour le Raspberry Pi :
   ```bash
   sudo apt update && sudo apt upgrade -y
   ```
2. Installer les dÃ©pendances requises :
   ```bash
   sudo apt install git cmake libusb-1.0-0-dev libturbojpeg0-dev libglfw3-dev
   ```
3. Cloner le dÃ©pÃ´t de **libfreenect** :
   ```bash
   git clone https://github.com/OpenKinect/libfreenect.git
   ```
4. Compiler et installer :
   ```bash
   cd libfreenect
   mkdir build && cd build
   cmake ..
   make
   sudo make install
   sudo ldconfig
   ```

---

## ğŸƒ 3. Tester le Kinect
VÃ©rifier si le Kinect est bien dÃ©tectÃ© :
```bash
lsusb
```
Vous devriez voir une ligne contenant **Microsoft Corp. Xbox NUI Camera**.

Tester lâ€™affichage vidÃ©o :
```bash
freenect-glview
```
Si tout fonctionne, une fenÃªtre avec lâ€™image de la camÃ©ra devrait apparaÃ®tre.

---

## ğŸ” 4. Acquisition et traitement des donnÃ©es
1. **Acquisition des donnÃ©es** :
   - Utiliser le **Kinect** pour capturer les mouvements des mains et du corps.
   - Extraire les coordonnÃ©es des articulations via **OpenNI** ou **libfreenect**.

### ğŸ“Œ Extraction avec libfreenect (Kinect v1)
libfreenect permet dâ€™accÃ©der aux donnÃ©es de profondeur mais ne gÃ¨re pas directement le suivi du squelette.

#### **Activer le capteur de profondeur**
```bash
freenect-glview
```

#### **RÃ©cupÃ©rer les donnÃ©es brutes en Python**
```python
import freenect
import cv2
import numpy as np

def get_depth():
    depth, _ = freenect.sync_get_depth()
    depth = depth.astype(np.uint8)  # Conversion pour affichage
    return depth

while True:
    depth_frame = get_depth()
    cv2.imshow('Kinect Depth', depth_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cv2.destroyAllWindows()
```
---

### ğŸ“Œ Extraction avec OpenNI (suivi des articulations)
OpenNI permet le suivi des squelettes avec NiTE.

#### **Installation dâ€™OpenNI et NiTE**
```bash
sudo apt install libopenni2-dev
```

#### **Code Python pour rÃ©cupÃ©rer les articulations**
```python
from openni import openni2
import numpy as np

openni2.initialize()
dev = openni2.Device.open_any()
depth_stream = dev.create_depth_stream()
depth_stream.start()

while True:
    frame = depth_stream.read_frame()
    frame_data = frame.get_buffer_as_uint16()
    depth_array = np.array(frame_data).reshape(480, 640)
    print("Profondeur Ã  un point (320, 240) :", depth_array[240, 320])
```
Ce script rÃ©cupÃ¨re la profondeur Ã  un point donnÃ©.

---

2. **Traitement des gestes** :
   - Identifier et classifier les mouvements en signes avec **une IA basÃ©e sur un modÃ¨le de reconnaissance gestuelle** (TensorFlow, Mediapipe, etc.).
   - EntraÃ®ner un modÃ¨le sur un dataset de langue des signes (ex: **RWTH-PHOENIX-Weather** pour la LSF).

#### ğŸ“Œ Utilisation de Mediapipe pour suivre les mains
```python
import cv2
import mediapipe as mp

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)

cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_frame)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            for idx, landmark in enumerate(hand_landmarks.landmark):
                h, w, _ = frame.shape
                cx, cy = int(landmark.x * w), int(landmark.y * h)
                cv2.circle(frame, (cx, cy), 5, (255, 0, 0), -1)

    cv2.imshow("Hand Tracking", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

3. **Traduction en texte** :
   - Convertir les gestes reconnus en texte via un moteur NLP (Natural Language Processing).
   - Structurer la phrase pour une meilleure lisibilitÃ©.

4. **Affichage / SynthÃ¨se vocale** :
   - Afficher le texte traduit sur un Ã©cran.
   - Ajouter une synthÃ¨se vocale pour une lecture audio (ex: **espeak** sur Raspberry Pi).

---

## ğŸ“Œ Remarques
- **Kinect v2** nÃ©cessite **libfreenect2**, qui est plus complexe Ã  installer sur Raspberry Pi.
- Pour le **suivi des squelettes**, il faut utiliser OpenNI ou NiTE (plus difficile Ã  configurer sur Pi).

ğŸ“¢ **Si vous voulez utiliser Kinect pour un projet spÃ©cifique, nâ€™hÃ©sitez pas Ã  poser vos questions !** ğŸ˜Š


